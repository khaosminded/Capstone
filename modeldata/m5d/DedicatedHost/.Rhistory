sample.mean[i]=mean(samp)                  # and compute each sample's mean.
}
mean(sample.mean)    # Compare mean of sample means with pop mean, 0.9955118.
sd(sample.mean)      # Note the difference of sd of sample means with sd of pop.
pop.sd/sqrt(sample.size) # But compare sd of sample means with pop.sd/root(n) .
par(mfrow = c(1,2))
hist(sample.mean,breaks=40)
qqnorm(sample.mean)
qqline(sample.mean)
hist(pop,breaks=400)           # Check it out. Very non-normal.
par(mfrow = c(1, 1))
n.trial = 10000                  # Take 10000 samples of
sample.size = 5                  # size 5 (i.e., small) from the population.
par(mfrow = c(1, 1))
hist(pop,breaks=400)           # Check it out. Very non-normal.
par(mfrow = c(1, 5))
hist(pop,breaks=400)
hist(pop,breaks=400, xlim = c(0, 10))
par(mfrow = c(1, 5))
hist(pop,breaks=400, xlim = c(0, 10))
par(mfrow = c(1, 5))
hist(pop,breaks=400, xlim = c(0, 10), ylim = c(0, 800))
n.trial = 10000                  # Take 10000 samples of
sample.size = 5                  # size 5 (i.e., small) from the population.
sample.mean = numeric(n.trial)   # Space for storing the 10000 sample means.
for (i in 1:n.trial) {
samp = sample(pop, sample.size, replace=T) # Take a sample (with replacement)
sample.mean[i]=mean(samp)                  # and compute each sample's mean.
}
mean(sample.mean)    # Compare mean of sample means with pop mean, 0.9955118.
sd(sample.mean)      # Note the difference of sd of sample means with sd of pop.
pop.sd/sqrt(sample.size) # But compare sd of sample means with pop.sd/root(n)
hist(sample.mean,breaks=40, xlim = c(0, 10), ylim = c(0, 800))
sample.size = 10                  # size 5 (i.e., small) from the population.
sample.mean = numeric(n.trial)   # Space for storing the 10000 sample means.
for (i in 1:n.trial) {
samp = sample(pop, sample.size, replace=T) # Take a sample (with replacement)
sample.mean[i]=mean(samp)                  # and compute each sample's mean.
}
mean(sample.mean)    # Compare mean of sample means with pop mean, 0.9955118.
sd(sample.mean)      # Note the difference of sd of sample means with sd of pop.
pop.sd/sqrt(sample.size) # But compare sd of sample means with pop.sd/root(n)
hist(sample.mean,breaks=40, xlim = c(0, 10), ylim = c(0, 800))
sample.size = 50                  # size 5 (i.e., small) from the population.
sample.mean = numeric(n.trial)   # Space for storing the 10000 sample means.
for (i in 1:n.trial) {
samp = sample(pop, sample.size, replace=T) # Take a sample (with replacement)
sample.mean[i]=mean(samp)                  # and compute each sample's mean.
}
mean(sample.mean)    # Compare mean of sample means with pop mean, 0.9955118.
sd(sample.mean)      # Note the difference of sd of sample means with sd of pop.
pop.sd/sqrt(sample.size) # But compare sd of sample means with pop.sd/root(n)
hist(sample.mean,breaks=40, xlim = c(0, 10), ylim = c(0, 800))
sample.size = 100                  # size 5 (i.e., small) from the population.
sample.mean = numeric(n.trial)   # Space for storing the 10000 sample means.
for (i in 1:n.trial) {
samp = sample(pop, sample.size, replace=T) # Take a sample (with replacement)
sample.mean[i]=mean(samp)                  # and compute each sample's mean.
}
mean(sample.mean)    # Compare mean of sample means with pop mean, 0.9955118.
sd(sample.mean)      # Note the difference of sd of sample means with sd of pop.
pop.sd/sqrt(sample.size) # But compare sd of sample means with pop.sd/root(n)
hist(sample.mean,breaks=40, xlim = c(0, 10), ylim = c(0, 800))
n.trial = 10000                 # Take 10000 samples of
sample.size = 10                # size 10 (i.e., small) from the population.
sample.median = numeric(n.trial) # Space for storing the 10000 sample medians.
for (i in 1:n.trial) {
samp = sample(pop, sample.size, replace=T) # Take a sample (with replacement)
sample.median[i]=median(samp)              # and compute each sample's MEDIAN.
}
hist(pop,col="skyblue")
median(pop)
par(mfrow = c(1,2))
hist(pop,col="skyblue")
median(pop)
abline(v=median(pop),col="green",lwd=2)
sd(pop)
hist(sample.median,col="skyblue")
abline(v=mean(sample.medians))
mean(sample.median)  # Compare MEAN of sample MEDIANS with pop median.
sd(sample.median)    # Note the difference of sd of sample means with sd of pop.
hist(sample.median,breaks=40)
qqnorm(sample.median, x = mean(sample.meidan), y = sd(sample.median))
abline(x = mean(sample.meidan), y = sd(sample.median))
hist(sample.median,breaks=40)
qqnorm(sample.median)
par(mfrow = c(1,2))
hist(sample.median,breaks=40)
qqnorm(sample.median)
abline(x = mean(sample.median), y = sd(sample.median))
abline(x = mean(sample.median), y = sd(sample.median))
abline(v = mean(sample.median), y = sd(sample.median))
abline?
abline?
abline(a = mean(sample.median), b = sd(sample.median), col = "purple")
rm(list = ls(all = TRUE))
x = c(418, 421, 421, 422, 425, 427, 431, 434, 437, 439, 446, 447, 448, 453, 454, 463, 465)
x.mean = mean(x)
x.sd = sd(x)
y = c(429, 430, 430, 431, 436, 437, 440, 441, 445, 446, 447)
y.mean = mean(y)
y.sd = sd(y)
par(mfrow = c(1,2))
hist(y)
qqnorm(y)
y
par(mfrow = c(1,3))
boxplot(x)
hist(x)
qqnorm(x)
par(mfrow = c(1,2))
boxplot(x)
boxplot(y)
par(mfrow = c(1,2))
hist(y)
qqnorm(y)
setwd("/home/ravschoo/Documents/Desktop/TMATH390/TMATH390_SP17")
list.files()
library(readr)
KWayMergeData_r <- read_csv("/home/ravschoo/Documents/Desktop/TMATH390/KWayMergeData_r.csv")
x = KWayMergeData_r$`k = 2`
y = KWayMergeData_r$`k = 3`
fivenum(x)
fivenum(y)
par(mfrow = c(1, 3))
boxplot(x, ylab = "Time in milliseconds for k = 2")
boxplot(y, ylab = "Time in milliseconds for k = 3")
plot(x, y, xlab = "Time in millisecond for k = 2", ylab = "Time in milliseconds for k = 3")
sar <- read_tsv(activity.tsv)
sar <- read_tsv(activity.tsv)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
#I need to take it each benchmarks data set
ycruncher <- read.table("./ycruncher_standardized.csv", sep=",", header=TRUE)
sysbench <- read.table("./sysbench_standardized.csv", sep=",", header=TRUE)
pgbench <- read.table("./pgbench_standardized.csv", sep=",", header=TRUE)
iperf <- read.table("./iperf_standardized.csv", sep=",", header=TRUE)
#Take just 3 observations for each set and vid
new_sysbench <- data.frame(sysbench=vector(), setId=vector(), vmId=vector())
for (setid in 0:47) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- sysbench[(sysbench$setId == setid & sysbench$vmId == vmid), ][1:3, ]
result <- na.omit(result)
result$sysbench <- as.numeric(gsub("s", "", unlist(result$sysbench)))
new_sysbench <- rbind(new_sysbench, result)
}
}
new_ycruncher <- data.frame(ycruncher=vector(), setId=vector(), vmId=vector())
for (setid in 0:47) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- ycruncher[(ycruncher$setId == setid & ycruncher$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_ycruncher <- rbind(new_ycruncher, result)
}
}
new_iperf <- data.frame(iperf=vector(), setId=vector(), vmId=vector())
for (setid in 0:47) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- iperf[(iperf$setId == setid & iperf$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_iperf <- rbind(new_iperf, result)
}
}
View(new_iperf)
View(new_sysbench)
new_iperf <- data.frame(iperf=vector(), setId=vector(), vmId=vector())
for (setid in 1:48) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- iperf[(iperf$setId == setid & iperf$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_iperf <- rbind(new_iperf, result)
}
}
new_sysbench$setId <- new_sysbench$setId * -1 + 48
new_sysbench <- new_sysbench[order(new_sysbench$setId), ]
print(head(new_sysbench))
print(head(new_iperf))
print(head(new_ycruncher))
new_ycruncher$setId <- new_ycruncher$setId * -1 + 48
new_ycruncher <- new_ycruncher[order(new_ycruncher$setId), ]
print(head(new_ycruncher))
pgbench$setId <- pgbench$setId * -1 + 48
pgbench <- pgbench[order(pgbench$setId), ]
print(head(pgbench))
#Now fix the vmids
pgbench$vmId <- new_iperf$vmId
new_sysbench$vmId <- new_iperf$vmId
new_ycruncher$vmId <- new_iperf$vmId
#Notice that as of now the vmids are not in the right order.
#Should start with 1 and go consecutively, will fix later.
print(head(new_sysbench))
print(head(new_ycruncher))
print(head(pgbench))
#All data sets should have same dimensions
print(dim(new_iperf))
print(dim(pgbench))
print(dim(new_sysbench))
print(dim(new_ycruncher))
#Put all data into a dataframe
globalMerged = data.frame()
for (set in 1:48) {
for (vm in 1:set) {
a = new_iperf[new_iperf$setId == set & new_iperf$vmId == vm,]
b = new_sysbench[new_sysbench$setId == set & new_sysbench$vmId == vm,]
c = new_ycruncher[new_ycruncher$setId == set & new_ycruncher$vmId == vm,]
d = pgbench[pgbench$setId == set & pgbench$vmId == vm,]
merged = cbind(a["i_perf"],b["sysbench"],c["yCruncher"],d["pgbench"],rep.int(set, 3), rep.int(vm, 3))
globalMerged = rbind(globalMerged, merged)
}
}
View(globalMerged)
#write it out as merged.csv.
write.csv(globalMerged,"./merged.csv",row.names=FALSE, col.names=FALSE)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Loading the dplyr package
library(dplyr)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
View(wholeSet)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
wholeSet = read.csv("./merged.csv")
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
print(modelRandomForest)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
#I need to take it each benchmarks data set
ycruncher <- read.table("./ycruncher_standardized.csv", sep=",", header=TRUE)
sysbench <- read.table("./sysbench_standardized.csv", sep=",", header=TRUE)
pgbench <- read.table("./pgbench_standardized.csv", sep=",", header=TRUE)
iperf <- read.table("./iperf_standardized.csv", sep=",", header=TRUE)
#Take just 3 observations for each set and vid
new_sysbench <- data.frame(sysbench=vector(), setId=vector(), vmId=vector())
for (setid in 0:47) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- sysbench[(sysbench$setId == setid & sysbench$vmId == vmid), ][1:3, ]
result <- na.omit(result)
result$sysbench <- as.numeric(gsub("s", "", unlist(result$sysbench)))
new_sysbench <- rbind(new_sysbench, result)
}
}
new_ycruncher <- data.frame(ycruncher=vector(), setId=vector(), vmId=vector())
for (setid in 0:47) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- ycruncher[(ycruncher$setId == setid & ycruncher$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_ycruncher <- rbind(new_ycruncher, result)
}
}
new_iperf <- data.frame(iperf=vector(), setId=vector(), vmId=vector())
for (setid in 1:48) {
for (vmid in 1:48) {
#for each set and vm combination we need 3 observations
result <- iperf[(iperf$setId == setid & iperf$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_iperf <- rbind(new_iperf, result)
}
}
View(new_sysbench)
View(new_sysbench)
new_sysbench$setId <- new_sysbench$setId * -1 + 48
new_sysbench <- new_sysbench[order(new_sysbench$setId), ]
#Notice that as of now the vmids are not in the right order.
#Should start with 1 and go consecutively, will fix later.
print(head(new_sysbench))
#This data set looks good already
print(head(new_iperf))
View(new_ycruncher)
new_ycruncher$setId <- new_ycruncher$setId * -1 + 48
new_ycruncher <- new_ycruncher[order(new_ycruncher$setId), ]
print(head(new_ycruncher))
View(pgbench)
pgbench$setId <- pgbench$setId * -1 + 48
pgbench <- pgbench[order(pgbench$setId), ]
print(head(pgbench))
#Now fix the vmids
pgbench$vmId <- new_iperf$vmId
new_sysbench$vmId <- new_iperf$vmId
new_ycruncher$vmId <- new_iperf$vmId
View(new_sysbench)
View(new_ycruncher)
#All data sets should have same dimensions
print(dim(new_iperf))
print(dim(pgbench))
print(dim(new_sysbench))
print(dim(new_ycruncher))
#Put all data into a dataframe
globalMerged = data.frame()
for (set in 1:48) {
for (vm in 1:set) {
a = new_iperf[new_iperf$setId == set & new_iperf$vmId == vm,]
b = new_sysbench[new_sysbench$setId == set & new_sysbench$vmId == vm,]
c = new_ycruncher[new_ycruncher$setId == set & new_ycruncher$vmId == vm,]
d = pgbench[pgbench$setId == set & pgbench$vmId == vm,]
merged = cbind(a["i_perf"],b["sysbench"],c["yCruncher"],d["pgbench"],rep.int(set, 3), rep.int(vm, 3))
globalMerged = rbind(globalMerged, merged)
}
}
View(globalMerged)
colnames(globalMerged)[colnames(globalMerged)=="rep.int(set, 3)"] <- set
colnames(globalMerged)[colnames(globalMerged)=="rep.int(set, 3)"] <- "set"
colnames(globalMerged)[colnames(globalMerged)=="48"] <- "set"
colnames(globalMerged)[colnames(globalMerged)=="rep.int(vm, 3)"] <- "vm"
#write it out as merged.csv.
write.csv(globalMerged,"./merged.csv",row.names=FALSE, col.names=FALSE)
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Loading the dplyr package
library(dplyr)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~i_perf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
print(modelRandomForest)
modelRandomForest <- randomForest(
formula,
data=train)
#Clear workspace
rm(list=ls())
wholeSet = read.csv("./merged.csv")
library(randomForest)
library(rpart)
str(wholeSet)
# Loading the dplyr package
library(dplyr)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
print(modelRandomForest)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
set.seed(100)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Loading the dplyr package
library(dplyr)
formula = setId~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(formula, data=wholeSet, ntree=2000, na.action=na.exclude)
# Importance of each predictor.
print(importance(modelRandomForest,type = 2))
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
summary (mlr_model) # model summary
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
predictions <- predict(mlr_model, wholeSet)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
#calculate the rmse
library("ModelMetrics")
predictions_rmse <- rmse(predictions, wholeSet$setId)
predictions_rmse
metrics_mae <- mae(predictions, wholeSet$setId)
metrics_mae
wholeSet$setId[1]
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - wholeSet$setId[i])
}
abs_error
sum(abs_error) / mean(abs_error)
sum(abs_error) / 1176
write.csv(abs_error, "./unstratified_abs_error.csv")
plot(predictions, wholeSet$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="Unstratified MLR")
abline(a=0,b=1)
View(wholeSet)
#This should give us the mean of the abs error for predictions that should be 48 vms.
mean(abs_error[1:48])
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_treated.rds")
print(mlr_model)
summary (mlr_model) # model summary
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
library("ModelMetrics")
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
library("ModelMetrics")
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
library(splitstackshape)
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
#Scale data
#Scale data to make it normally distributed
#iperf_log <- log(strat_data$iperf)
#summary(iperf_log)
iperf_scaled <- scale(strat_data$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(strat_data$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(strat_data$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(strat_data$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
strat_data$iperf <- iperf_scaled
strat_data$pgbench <- pgbench_scaled
strat_data$sysbench <- sysbench_scaled
strat_data$ycruncher <- ycruncher_scaled
predictions <- predict(mlr_model, strat_data)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
predictions_rmse <- rmse(predictions, strat_data$setId)
predictions_rmse
metrics_mae <- mae(predictions, strat_data$setId)
metrics_mae
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - wholeSet$setId[i])
}
View(strat_data)
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - strat_data$setId[i])
}
#This should give us the mean of the abs error for predictions that should be 48 vms.
mean(abs_error[1:3])
write.csv(abs_error, "./treated_mlr_abs_error.csv")
sum(abs_error) / 141
abs_error
#This should give us the mean of the abs error for predictions that should be 48 vms.
mean(abs_error[1:3])
abs_error[1:3] / 3
sum(abs_error[1:3]) / 3
